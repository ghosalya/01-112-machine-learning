{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Homework 3\n",
    "\n",
    "**Gede Ria Ghosalya - 1001841**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![pic](rat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "learn_rate = -0.001\n",
    "u = theano.shared(np.ones((3,1)), name='u')\n",
    "v = theano.shared(np.ones((3,1)), name='v')\n",
    "tgt = numpy.array()\n",
    "tgt[0] = [0,1,0]\n",
    "tgt[1] = [1,0,1]\n",
    "tgt[2] = [0,1,2]\n",
    "\n",
    "risk = (T.dot(u, v.T)-target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "csv = 'https://www.dropbox.com/s/wt45tvn9ig3o7vu/kernel.csv?dl=1'\n",
    "data = np.genfromtxt(csv, delimiter=',')\n",
    "X = data[:,1:]\n",
    "Y = data [:,0]\n",
    "plt.scatter(X[:,0],X[:,1],c=Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**a)** Use the `sklearn.svm.SVC` module to train a kernel support vector machine via the radial basis kernel. Set gamma to 0.5 and `kernel` to `rbf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "dis_svc = SVC(gamma=0.5, kernel='rbf')\n",
    "dis_svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**b)** Evaluate the kernel SVM's decision function. You may use the `decision_function` method in `SVC`. Write a function that takes coordinates `x1, x2` and the SVC object `clf`, and return the value of decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = dis_svc\n",
    "\n",
    "def decision(x1, x2, clf):\n",
    "    x = np.array([[x1, x2]])\n",
    "    val = clf.decision_function(x)\n",
    "    return val[0]\n",
    "\n",
    "decision(3,5,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**c)** Use the following code to visualize the classifier and the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vdecision = np.vectorize(decision,excluded=[2])\n",
    "x1list = np.linspace(-8.0, 8.0, 100)\n",
    "x2list = np.linspace(-8.0, 8.0, 100)\n",
    "X1, X2 = np.meshgrid(x1list, x2list)\n",
    "Z = vdecision(X1, X2, clf)\n",
    "cp = plt.contourf(X1, X2, Z)\n",
    "plt.colorbar(cp)\n",
    "plt.scatter(X[:,0], X[:,1], c=Y, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fmin_l_bfgs_b as minimize\n",
    "\n",
    "from utils import normalize, tile_raster_images, sigmoid\n",
    "from utils import ravelParameters, unravelParameters\n",
    "from utils import initializeParameters\n",
    "from utils import computeNumericalGradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nV = 8*8\n",
    "nH = 25\n",
    "dW = 0.0001\n",
    "sW = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "npy = 'images.npy'\n",
    "X = normalize(np.load(npy))\n",
    "plt.imshow(tile_raster_images(X=X, img_shape=(8,8), tile_shape=(5,5), \n",
    "                              tile_spacing=(1,1)),\n",
    "           cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**a)** We implement the function which computes the cost and the gradient of the sparse autoencoder. This function will be passed to an optimization engine, together with the `theta` vector that contains the current state of all the model parameters. The first step of the function is therefore to unpack the `theta` vector into _W1, W2, b1, b2_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sparseAutoencoderCost(theta, nV, nH, dW, sW, X):\n",
    "    W1, W2, b1, b2 = unravelParameters(theta, nH, nV)\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    z2 = np.dot(X,W1) + np.dot(np.ones((n,1)),b1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    z3 = np.dot(a2,W2) + np.dot(np.ones((n,1)),b2.T)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    eps = a3-X\n",
    "    loss = norm(eps)**2/(2*n)\n",
    "    decay = 0.5*(norm(W1)**2+norm(W2)**2)\n",
    "    \n",
    "    #compute sparsity terms and total cost\n",
    "    rho = 0.01\n",
    "    a2mean = np.mean(a2, axis=0).reshape(nH, 1)\n",
    "    kl_first = rho*np.log(rho/a2mean)\n",
    "    kl_last = (1-rho)*np.log((1-rho)/(1-a2mean))\n",
    "    kl = np.sum(kl_first + kl_last)\n",
    "    dkl = -rho/a2mean+(1-rho)/(1-a2mean)\n",
    "    cost = loss+dW*decay+sW*kl\n",
    "    \n",
    "    d3 = eps*a3*(1-a3)\n",
    "    d2 = (sW*dkl.T+np.dot(d3, W2.T))*a2*(1-a2)\n",
    "    W1grad = np.dot(X.T,d2)/n + dW*W1\n",
    "    W2grad = np.dot(a2.T,d3)/n + dW*W2\n",
    "    b1grad = np.dot(d2.T,np.ones((n,1)))/n\n",
    "    b2grad = np.dot(d3.T,np.ones((n,1)))/n\n",
    "    \n",
    "    grad = ravelParameters(W1grad,W2grad,\n",
    "                           b1grad, b2grad)\n",
    "    print(' .',end=\"\")\n",
    "    return cost, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = initializeParameters(nH, nV)\n",
    "cost, grad = sparseAutoencoderCost(theta, nV, nH, dW, sW, X)\n",
    "print(cost,grad)\n",
    "#print(np.ones(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**b)** Compare the backdrop gradient in `sparseAutoencoderCost` with the gradient computed numerically from the cost. The relative difference should be less than 10e-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\nComparing numerical gradient with backdrop gradient')\n",
    "num_coords = 5\n",
    "indices = np.random.choice(theta.size, num_coords, replace=False)\n",
    "numgrad = computeNumericalGradient(lambda t: \n",
    "                                       sparseAutoencoderCost(t,nV,nH,dW,sW,X)[0],\n",
    "                                   theta,indices)\n",
    "subnumgrad = numgrad[indices]\n",
    "subgrad = grad[indices]\n",
    "diff = norm(subnumgrad-subgrad)/norm(subnumgrad+subgrad)\n",
    "print('\\n',np.array([subnumgrad,subgrad]).T)\n",
    "print('Relative difference: ', diff)\n",
    "\n",
    "if diff < 10**(-9):\n",
    "    print(\"small enough!\")\n",
    "else:\n",
    "    print(\"NOOOOO!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**c)** Finally, run the following code to train the deep neural network and to visualize the features learnt by the autoencoder. The optimization takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\nTraining neural network')\n",
    "theta = initializeParameters(nH,nV)\n",
    "opttheta,cost,msg = minimize(sparseAutoencoderCost,\n",
    "                             theta,fprime=None,maxiter=400,\n",
    "                             args=(nV,nH,dW,sW,X))\n",
    "W1,W2,b1,b2 = unravelParameters(opttheta,nH,nV)\n",
    "plt.imshow(tile_raster_images(X=W1.T, img_shape=(8,8),\n",
    "                              tile_shape=(5,5),tile_spacing=(1,1)),\n",
    "           cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Q4. DataSpark Challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataspark2.csv')\n",
    "data = data.drop(['seqid','index','acc','dir','spd'],\n",
    "                 axis=1)\n",
    "print(data.info())\n",
    "plt.scatter(data['lon'],data['lat'],marker='.')\n",
    "plt.show()\n",
    "#data = data.sample(frac=0.05, random_state=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['date'] = pd.DatetimeIndex(data['date']).round('5min')\n",
    "data = data.groupby(['userid','date']).mean().reset_index()\n",
    "print(data.info())\n",
    "plt.scatter(data['lon'],data['lat'],marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**a)** Cluster the GPS locations for all users to find commonly visited places. Us the 'elbow' method to find a suitable number of clusters. Sample the data to improve speed. Write your guess for the number of clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "smp = data[['lat','lon']].sample(n=3000,random_state=200)\n",
    "score = []\n",
    "cls_range = list(range(10,150,10))\n",
    "for num_cls in cls_range:\n",
    "    n_cluster = KMeans(n_clusters=num_cls).fit(smp)\n",
    "    score.append(n_cluster.inertia_)\n",
    "plt.plot(cls_range,score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My guess is that we have 20 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**b)** Visualize the trained centroids. In the code below, `centroid` is a numpy array where each row consists of the latitude of some centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cluster = 20\n",
    "data_latlon = data[['lat','lon']]\n",
    "decided_kmean = KMeans(n_clusters=num_cluster).fit(data_latlon)\n",
    "centroids = decided_kmean.cluster_centers_\n",
    "plt.scatter(centroids[:,1],centroids[:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**c)** Compute the speeds at which each user is travelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "for u in data['userid'].unique():\n",
    "    user = data[data['userid']==u]\n",
    "    date = pd.DatetimeIndex(user['date'])\n",
    "    hour = (date-date[0])/np.timedelta64(1,'h')\n",
    "    hourfor = np.append(hour[1:], hour[-1])\n",
    "    dur = hourfor - hour\n",
    "    \n",
    "    latlon = user[['lat','lon']].get_values()\n",
    "    latlonfor = np.vstack([latlon[1:],latlon[-1]])\n",
    "    displacement = (latlonfor - latlon)**2\n",
    "    disp_p = (displacement[:,0] + displacement[:,1])**0.5\n",
    "    \n",
    "    speed = 111*disp_p/dur\n",
    "    data.loc[data['userid']==u,'speed'] = speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = data[data['speed']<1]\n",
    "plt.scatter(stop['lon'],stop['lat'],\n",
    "            c=np.log(stop['speed']+1),\n",
    "            marker='.')\n",
    "plt.show()\n",
    "print('Number of entries =',stop.shape[0])\n",
    "\n",
    "move = data[data['speed']>=1]\n",
    "plt.scatter(move['lon'],move['lat'],\n",
    "            c=np.log(move['speed']+1),\n",
    "            marker='.')\n",
    "plt.show()\n",
    "print('Number of entries =',move.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
