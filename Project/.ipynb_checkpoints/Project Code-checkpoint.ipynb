{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01.112 Machine Learning - Project\n",
    "**Gede Ria Ghosalya - 1001841**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Part II_\n",
    "+ Write a function that estimates the emission parameters from the training set using MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading file\n",
    "\n",
    "def read_labeled_file(filename):\n",
    "    result = []\n",
    "    singletweet = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line == \"\\n\":\n",
    "                result.append(singletweet)\n",
    "                singletweet = []\n",
    "            else:\n",
    "                singletweet.append(tuple(line.strip(\"\\n\").split(\" \")))\n",
    "    return result\n",
    "            \n",
    "# data = read_labeled_file(\"EN/train\")\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_emission_param(data):\n",
    "    label_to_word_count = {}\n",
    "    word_count = {}\n",
    "    label_count = {}\n",
    "    \n",
    "    for tweet in data:\n",
    "        for labeled_word in tweet:\n",
    "            if labeled_word[0] in word_count:\n",
    "                word_count[labeled_word[0]] += 1\n",
    "            else:\n",
    "                word_count[labeled_word[0]] = 1\n",
    "                \n",
    "            if labeled_word[1] in label_count:\n",
    "                label_count[labeled_word[1]] += 1\n",
    "            else:\n",
    "                label_count[labeled_word[1]] = 1\n",
    "                \n",
    "            if labeled_word in label_to_word_count:\n",
    "                label_to_word_count[labeled_word] += 1\n",
    "            else:\n",
    "                label_to_word_count[labeled_word] = 1\n",
    "                \n",
    "    emission_parameter = {k: label_to_word_count[k]/label_count[k[1]] \n",
    "                          for k in label_to_word_count}\n",
    "    \n",
    "    return word_count.keys(), label_count.keys(), emission_parameter\n",
    "\n",
    "# words, labels, param = estimate_emission_param(data)\n",
    "# print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+  One problem with estimating the emission parameters is that some words that appear in the\n",
    "test set do not appear in the training set. One simple idea to handle this issue is as follows. First,\n",
    "replace those words that appear less than k times in the training set with a special token #UNK#\n",
    "before training. This leads to a “modified training set”. We then use such a modified training set to\n",
    "train our model.\n",
    "During the testing phase, if the word does not appear in the “modified training set”, we replace that\n",
    "word with #UNK# as well.\n",
    "Set k to 3, implement this fix into your function for computing the emission parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supress_infrequent_words(data, k=3):\n",
    "    wordcount = {}\n",
    "    \n",
    "    #get word count\n",
    "    for tweet in data:\n",
    "        for labeled_word in tweet:\n",
    "            word = labeled_word[0]\n",
    "            if word in wordcount:\n",
    "                wordcount[word] += 1\n",
    "            else:\n",
    "                wordcount[word] = 1\n",
    "                \n",
    "    #generate new list\n",
    "    result = []\n",
    "    newtweet = []\n",
    "    for tweet in data:\n",
    "        for labeled_word in tweet:\n",
    "            word = labeled_word[0]\n",
    "            if wordcount[word] > k:\n",
    "                newtweet.append(labeled_word)\n",
    "            else:\n",
    "                label = labeled_word[1]\n",
    "                newtweet.append((\"#UNK#\",label))\n",
    "        result.append(newtweet)\n",
    "        newtweet = []\n",
    "        \n",
    "    return result\n",
    "\n",
    "# sdata = supress_infrequent_words(data)\n",
    "# words, labels, em_param = estimate_emission_param(sdata)\n",
    "# print(em_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Implement a simple sentiment analysis system that produces the tag for each word x in the sequence.\n",
    "For all the four datasets EN, FR, CN, and SG, learn these parameters with train, and evaluate your\n",
    "system on the development set dev.in for each of the dataset. Write your output to dev.p2.out\n",
    "for the four datasets respectively. Compare your outputs and the gold-standard outputs in dev.out\n",
    "and report the precision, recall and F scores of such a baseline system for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_sentiment_analysis(labels, param, word):\n",
    "    mle = (word, \"O\") #assuming label O for undiscovered word\n",
    "    mle_value = 0\n",
    "    for l in labels:\n",
    "        if (word, l) in param:\n",
    "            if param[(word, l)] > mle_value:\n",
    "                mle = (word, l)\n",
    "                mle_value = param[(word, l)]\n",
    "    return mle\n",
    "\n",
    "\n",
    "def write_simple_prediction(country, part, words, labels, param):\n",
    "    input_filename = country + \"/dev.in\"\n",
    "    output_filename = country + \"/dev.p\"+part+\".out\"\n",
    "    with open(input_filename, \"r\") as inputfile:\n",
    "        with open(output_filename, \"w\") as outputfile:\n",
    "            for line in inputfile:\n",
    "                if line.strip(\"\\n\") in words:\n",
    "                    pred = single_sentiment_analysis(labels, param, line.strip(\"\\n\"))\n",
    "                    outputfile.write(\" \".join(pred)+\"\\n\")\n",
    "                else:\n",
    "                    outputfile.write(\"#UNK# O\\n\")\n",
    "\n",
    "# write_simple_prediction(\"EN\",\"2\",\n",
    "#                         words, labels, em_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN part 2 done in 0.152399s\n",
      "EN part 2 done in 0.77301s\n",
      "SG part 2 done in 0.296096s\n",
      "FR part 2 done in 0.70273s\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "for c in [\"CN\", \"EN\", \"SG\", \"FR\"]:\n",
    "    start = datetime.now()\n",
    "    data = read_labeled_file(c+\"/train\")\n",
    "    sdata = supress_infrequent_words(data)\n",
    "    words, labels, em_param = estimate_emission_param(sdata)\n",
    "    write_simple_prediction(c,\"2\",\n",
    "                    words, labels, em_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 2 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_Part III_\n",
    "\n",
    "+ Write a function that estimates the transition parameters from the training set using MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_transition_parameter(data):\n",
    "    y0 = \"START\"\n",
    "    yn = \"STOP\"\n",
    "    label_to_label_count = {}\n",
    "    label_count = {y0: 0, yn: 0}\n",
    "    for tweet in data:\n",
    "        aptweet = tweet + [yn]\n",
    "        for i in range(len(aptweet)):\n",
    "            label = aptweet[i][1]\n",
    "            if label in label_count:\n",
    "                label_count[label] += 1\n",
    "            else:\n",
    "                label_count[label] = 1\n",
    "                \n",
    "            if i == 0:\n",
    "                label_count[y0] += 1\n",
    "                labelt = (y0, label)\n",
    "            else:\n",
    "                prevlabel = aptweet[i-1][1]\n",
    "                labelt = (prevlabel, label)\n",
    "            \n",
    "            if labelt in label_to_label_count:\n",
    "                label_to_label_count[labelt] += 1\n",
    "            else:\n",
    "                label_to_label_count[labelt] = 1\n",
    "    estimated_param = {k: label_to_label_count[k]/label_count[k[0]]\n",
    "                       for k in label_to_label_count}\n",
    "    return estimated_param\n",
    "\n",
    "# trans_param = estimate_transition_parameter(sdata)\n",
    "# print(trans_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use the estimated transition and emission parameters, implement the Viterbi algorithm. For all datasets, learn the model parameters with train. Run the Viterbi algorithm on the development\n",
    "set dev.in using the learned models, write your output to dev.p3.out for the four datasets\n",
    "respectively. Report the precision, recall and F scores of all systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of dictionary \n",
    "#   a dictionary consists of\n",
    "#    stage = int (stage 0, 1, 2 etc)\n",
    "#    cur_label = current state (X,Y etc)\n",
    "#    path = path up to this label (eg [START, X, Y])\n",
    "#    probability = float (0.23)\n",
    "        \n",
    "def predict_tag_sequence(word_sequence, \n",
    "                         labels,trans_param,em_param):\n",
    "    pi_dp = {(0, \"START\"):1}\n",
    "    labels = list(labels)\n",
    "    labels.append(\"START\")\n",
    "    \n",
    "    def viterbi_pi(stage, tag, word_sequence,\n",
    "                   labels, trans_param, em_param):\n",
    "        if stage == 0:\n",
    "            result = 1.0 if tag == \"START\" else 0.0\n",
    "            return result\n",
    "        elif stage >= len(word_sequence)+1:\n",
    "            max_weight = 0\n",
    "            for prev_tag in labels:\n",
    "                prev_cost = viterbi_pi(stage-1, prev_tag, word_sequence,\n",
    "                                      labels, trans_param, em_param)\n",
    "                trans_prob = trans_param[(prev_tag, tag)]\n",
    "                curr_weight = prev_cost*trans_prob\n",
    "                if max_weight < curr_weight:\n",
    "                    max_weight = curr_weight\n",
    "            pi_dp[(stage, tag)] = max_weight\n",
    "            return max_weight\n",
    "        else:\n",
    "            if (stage, tag) in pi_dp:\n",
    "                return pi_dp[(stage, tag)]\n",
    "            else:\n",
    "                max_weight = 0\n",
    "                for prev_tag in labels:\n",
    "                    prev_cost = viterbi_pi(stage-1, prev_tag, word_sequence,\n",
    "                                          labels, trans_param, em_param)\n",
    "                    if (prev_tag, tag) in trans_param:\n",
    "                        trans_prob = trans_param[(prev_tag, tag)]\n",
    "                    else:\n",
    "                        trans_prob = 0\n",
    "                    word = word_sequence[stage-1]\n",
    "                    if (word, tag) in em_param:\n",
    "                        em_prob = em_param[(word, tag)]\n",
    "                    else:\n",
    "                        em_prob = 0\n",
    "                    curr_weight = prev_cost*trans_prob*em_prob\n",
    "                    if max_weight < curr_weight:\n",
    "                        max_weight = curr_weight\n",
    "                pi_dp[(stage, tag)] = max_weight\n",
    "                return max_weight\n",
    "\n",
    "    \n",
    "    for i in range(len(word_sequence)):\n",
    "        for t in labels:\n",
    "            viterbi_pi(i, t, word_sequence, \n",
    "                       labels, trans_param, em_param)\n",
    "    \n",
    "    tag_seqr = [\"STOP\"]\n",
    "    \n",
    "    lenw = len(word_sequence)\n",
    "    for i in range(lenw+1):\n",
    "        max_tag = \"O\"\n",
    "        max_weight = 0\n",
    "        for tag in labels:\n",
    "            prev_prob = viterbi_pi(lenw-i, tag, word_sequence,\n",
    "                                   labels, trans_param, em_param)\n",
    "            next_tag = tag_seqr[-1]\n",
    "            if (tag, next_tag) in trans_param:\n",
    "                trans_prob = trans_param[(tag, next_tag)]\n",
    "            else:\n",
    "                trans_prob = 0\n",
    "            curr_weight = prev_prob*trans_prob\n",
    "            if max_weight < curr_weight:\n",
    "                max_weight = curr_weight\n",
    "                max_tag = tag\n",
    "        tag_seqr.append(max_tag)\n",
    "    return tag_seqr[::-1]\n",
    "\n",
    "\n",
    "# word_sequence = [\"we\",\"like\",\"the\",\"ambience\"]\n",
    "# print(predict_tag_sequence(word_sequence))\n",
    "# print(pi_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN part 3 done in 0.654624s\n",
      "EN part 3 done in 0.352353s\n",
      "SG part 3 done in 2.142802s\n",
      "FR part 3 done in 0.320225s\n"
     ]
    }
   ],
   "source": [
    "def write_hmm_prediction(country, part, prediction_function,\n",
    "                         words, labels, em_param, trans_param):\n",
    "    input_filename = country + \"/dev.in\"\n",
    "    output_filename = country + \"/dev.p\"+part+\".out\"\n",
    "    indata = []\n",
    "    with open(input_filename, \"r\") as infile:\n",
    "        indata = infile.read().strip('\\n').split('\\n\\n') #read and separate tweets\n",
    "    \n",
    "    with open(output_filename, \"w\") as outfile:\n",
    "        for tweet in indata:\n",
    "            word_sequence = tweet.split('\\n')\n",
    "            predicted_tag = prediction_function(word_sequence, labels, trans_param, em_param)\n",
    "            predicted_tag.remove(\"START\")\n",
    "            predicted_tag.remove(\"STOP\")\n",
    "            if len(word_sequence) != len(predicted_tag):\n",
    "                print(\"WARNING!! Different length {} / {}\"\\\n",
    "                      .format(word_sequence, tag_sequence))\n",
    "            for i in range(len(word_sequence)):\n",
    "                line = \"{} {}\\n\".format(word_sequence[i], predicted_tag[i])\n",
    "                outfile.write(line)\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "for c in [\"CN\", \"EN\", \"SG\", \"FR\"]:\n",
    "    start = datetime.now()\n",
    "    data = read_labeled_file(c+\"/train\")\n",
    "    sdata = supress_infrequent_words(data)\n",
    "    words, labels, em_param = estimate_emission_param(sdata)\n",
    "    trans_param = estimate_transition_parameter(sdata)\n",
    "    write_hmm_prediction(c,\"3\", predict_tag_sequence,\n",
    "                        words, labels, em_param, trans_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 3 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " Part IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Use the estimated transition and emission parameters, implement the alternative max-marginal decoding algorithm. Clearly describe the steps of your algorithm in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphas = {}\n",
    "\n",
    "def alpha_forward(tag, stage, word_sequence, alphas,\n",
    "                  trans_param=trans_param,\n",
    "                  labels=labels,\n",
    "                  em_param=em_param):\n",
    "    '''\n",
    "    Forward algorithm\n",
    "    '''\n",
    "    if stage <= 1:\n",
    "        if (\"START\", tag) in trans_param:\n",
    "            score = trans_param[(\"START\", tag)]\n",
    "        else:\n",
    "            score = 0\n",
    "        alphas[(tag, stage)] = score\n",
    "        return score\n",
    "    else:\n",
    "        if (tag, stage) in alphas:\n",
    "            return alphas[(tag, stage)]\n",
    "        score = 0\n",
    "        for t in labels:\n",
    "            prev_score = alpha_forward(t, stage-1, word_sequence, alphas)\n",
    "            if (t, tag) in trans_param:\n",
    "                trans_prob = trans_param[(t, tag)]\n",
    "            else:\n",
    "                trans_prob = 0\n",
    "            word = word_sequence[stage-2]\n",
    "            if (word, t) in em_param:\n",
    "                em_prob = em_param[(word, t)]\n",
    "            else:\n",
    "                em_prob = 0\n",
    "            curr_score = prev_score*trans_prob*em_prob\n",
    "            score += curr_score\n",
    "        alphas[(tag, stage)] = score\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas = {}\n",
    "\n",
    "def beta_back(tag, stage, word_sequence, betas,\n",
    "                  trans_param=trans_param,\n",
    "                  labels=labels,\n",
    "                  em_param=em_param):\n",
    "    '''\n",
    "    Backward algorithm\n",
    "    '''\n",
    "    if stage <= 1:\n",
    "        if (tag, \"STOP\") in trans_param:\n",
    "            score = trans_param[(tag, \"STOP\")]\n",
    "        else:\n",
    "            score = 0\n",
    "        betas[(tag, stage)] = score\n",
    "        return score\n",
    "    else:\n",
    "        if (tag, stage) in betas:\n",
    "            return betas[(tag, stage)]\n",
    "        score = 0\n",
    "        for t in labels:\n",
    "            prev_score = beta_back(t, stage-1, word_sequence, betas)\n",
    "            if (tag, t) in trans_param:\n",
    "                trans_prob = trans_param[(tag, t)]\n",
    "            else:\n",
    "                trans_prob = 0\n",
    "            word = word_sequence[stage-2]\n",
    "            if (word, tag) in em_param:\n",
    "                em_prob = em_param[(word, tag)]\n",
    "            else:\n",
    "                em_prob = 0\n",
    "            curr_score = prev_score*trans_prob*em_prob\n",
    "            score += curr_score\n",
    "        betas[(tag, stage)] = score\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN part 4 done in 0.380277s\n",
      "FR part 4 done in 0.332578s\n"
     ]
    }
   ],
   "source": [
    "def predict_tag_sequence_maxmarginal(word_sequence,\n",
    "                                    labels=labels,\n",
    "                                    trans_param=trans_param,\n",
    "                                    em_param=em_param):\n",
    "    '''\n",
    "    using max-marginal decoding algorithm\n",
    "    '''\n",
    "    alphas = {}\n",
    "    betas = {}\n",
    "    \n",
    "    tag_seqr = [\"STOP\"]\n",
    "    \n",
    "    lenw = len(word_sequence)\n",
    "    for i in range(lenw):\n",
    "        max_tag = \"O\"\n",
    "        max_weight = 0\n",
    "        for tag in labels:\n",
    "            alph = alpha_forward(tag, i, word_sequence, alphas,\n",
    "                                 labels=labels, em_param=em_param, \n",
    "                                 trans_param=trans_param)\n",
    "            beth = beta_back(tag, i, word_sequence, betas,\n",
    "                                 labels=labels, em_param=em_param, \n",
    "                                 trans_param=trans_param)\n",
    "            curr_weight = alph*beth\n",
    "            if curr_weight > max_weight:\n",
    "                max_weight = curr_weight\n",
    "                max_tag = tag\n",
    "        tag_seqr.append(max_tag)\n",
    "    tag_seqr.append(\"START\")\n",
    "    return tag_seqr[::-1]\n",
    "\n",
    "# word_sequence = [\"we\",\"like\",\"the\",\"ambience\"]\n",
    "# print(predict_tag_sequence_maxmarginal(word_sequence))\n",
    "\n",
    "for c in [\"EN\", \"FR\"]:\n",
    "    start = datetime.now()\n",
    "    data = read_labeled_file(c+\"/train\")\n",
    "    sdata = supress_infrequent_words(data)\n",
    "    words, labels, em_param = estimate_emission_param(sdata)\n",
    "    write_hmm_prediction(c,\"4\",\n",
    "                        predict_tag_sequence_maxmarginal,\n",
    "                        words, labels, em_param, trans_param)\n",
    "    end = datetime.now()\n",
    "    delt = end - start\n",
    "    print(\"{} part 4 done in {}.{}s\"\\\n",
    "          .format(c, delt.seconds, delt.microseconds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the training and development set, think of a better design for developing an\n",
    "improved sentiment analysis system for tweets using any model you like. Please explain clearly the\n",
    "method that you used for designing the new system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Attempt 1: Using dual-tagged HMM\n",
    "\n",
    "There is some rule to the tagging that is not\n",
    "covered by the HMM alone, specifically sentiment\n",
    "vs identity. For example, B-positive cannot be\n",
    "followed by I-negative. \n",
    "\n",
    "Hence, one tag class is sentiment, [positive, negative, neutral, none]\n",
    "and another  is identity [O, B, I]\n",
    "before the observation layer\n",
    "'''\n",
    "\n",
    "# first of all, we want to generate\n",
    "# properly-layered file\n",
    "\n",
    "def split_tag_layer(input_filename, output_filename):\n",
    "    with open(input_filename, \"r\") as infile:\n",
    "        with open(output_filename, \"w\") as outfile:\n",
    "            for inline in infile:\n",
    "                if inline == \"\\n\":\n",
    "                    outfile.write(inline)\n",
    "                    continue\n",
    "                    \n",
    "                line = inline.strip(\"\\n\").split(\" \")\n",
    "                if len(line) < 2:\n",
    "                    pass\n",
    "                elif \"positive\" in line[1]:\n",
    "                    line[1] = line[1][0]\n",
    "                    line.append(\"positive\")\n",
    "                elif \"negative\" in line[1]:\n",
    "                    line[1] = line[1][0]\n",
    "                    line.append(\"negative\")\n",
    "                elif \"neutral\" in line[1]:\n",
    "                    line[1] = line[1][0]\n",
    "                    line.append(\"neutral\")\n",
    "                else:\n",
    "                    line.append(\"none\")\n",
    "                \n",
    "                string = \" \".join(line) + \"\\n\"\n",
    "                outfile.write(string)\n",
    "                \n",
    "split_tag_layer(\"EN/train\", \"EN/trainl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Now that the file is tagged properly,\n",
    "we can generate estimated parameters\n",
    "\n",
    "Note that while the identity tags [O, B, I] are ordered,\n",
    "sentiments are not - they are not always associated with \n",
    "the identified phares\n",
    "'''\n",
    "\n",
    "def read_splitlabel_file(filename):\n",
    "    sm_emparam = []\n",
    "    sm_tweet = []\n",
    "    id_emparam = []\n",
    "    id_tweet = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line == \"\\n\":\n",
    "                id_emparam.append(id_tweet)\n",
    "                id_tweet = []\n",
    "                sm_emparam.append(sm_tweet)\n",
    "                sm_tweet = []\n",
    "            else:\n",
    "                linetags = line.strip(\"\\n\").split(\" \")\n",
    "                id_tweet.append(tuple([linetags[0], linetags[1]]))\n",
    "                sm_tweet.append(tuple([linetags[0], linetags[2]]))\n",
    "    return sm_emparam, id_emparam\n",
    "\n",
    "sm_data, id_data = read_splitlabel_file(\"EN/trainl\")\n",
    "# print(sm_param)\n",
    "# print(id_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Once data is extracted, we can estimate\n",
    "identity (and sentiment) tags with normal HMM methods\n",
    "'''\n",
    "\n",
    "words, labels, id_emparam = estimate_emission_param(id_data)\n",
    "id_trans_param = estimate_transition_parameter(id_data)\n",
    "swords, slabels, sm_emparam = estimate_emission_param(sm_data)\n",
    "sm_trans_param = estimate_transition_parameter(sm_data)\n",
    "\n",
    "def predict_dualtag_sequence(word_sequence, labels,\n",
    "                            id_emparam, id_transparam,\n",
    "                            sm_emparam, sm_transparam):\n",
    "    id_tagsequence = predict_tag_sequence(word_sequence, labels,\n",
    "                                          id_transparam, id_emparam)\n",
    "    sm_tagsequence = predict_tag_sequence(word_sequence, labels,\n",
    "                                          sm_transparam, sm_emparam)\n",
    "    # we do some cleanup\n",
    "    # because in the original, only identities have sentiments\n",
    "    # but the prediction might not be the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
